{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Install requirements and import all necessary packages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1qN3Ua3r3cYW",
        "outputId": "592b7df5-118f-4c6f-fc21-bc3b21f3fd76"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 1)) (2.2.5)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 2)) (14.0.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 3)) (1.5.3)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 4)) (4.37.2)\n",
            "Collecting langchain (from -r requirements.txt (line 5))\n",
            "  Downloading langchain-0.1.9-py3-none-any.whl (816 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m817.0/817.0 kB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: huggingface_hub in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (0.20.3)\n",
            "Collecting lancedb (from -r requirements.txt (line 7))\n",
            "  Downloading lancedb-0.5.7-py3-none-any.whl (115 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m115.1/115.1 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting rank_bm25 (from -r requirements.txt (line 8))\n",
            "  Downloading rank_bm25-0.2.2-py3-none-any.whl (8.6 kB)\n",
            "Collecting sentence-transformers (from -r requirements.txt (line 9))\n",
            "  Downloading sentence_transformers-2.4.0-py3-none-any.whl (149 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m149.5/149.5 kB\u001b[0m \u001b[31m21.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes (from -r requirements.txt (line 10))\n",
            "  Downloading bitsandbytes-0.42.0-py3-none-any.whl (105.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.0/105.0 MB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate (from -r requirements.txt (line 11))\n",
            "  Downloading accelerate-0.27.2-py3-none-any.whl (279 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.0/280.0 kB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchainhub (from -r requirements.txt (line 12))\n",
            "  Downloading langchainhub-0.1.14-py3-none-any.whl (3.4 kB)\n",
            "Collecting llama_cpp_python (from -r requirements.txt (line 13))\n",
            "  Downloading llama_cpp_python-0.2.52.tar.gz (36.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m36.8/36.8 MB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (3.0.1)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (3.1.3)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->-r requirements.txt (line 1)) (8.1.7)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow->-r requirements.txt (line 2)) (1.25.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->-r requirements.txt (line 3)) (2023.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (3.13.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (23.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (0.4.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers->-r requirements.txt (line 4)) (4.66.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (2.0.27)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (3.9.3)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (4.0.3)\n",
            "Collecting dataclasses-json<0.7,>=0.5.7 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading dataclasses_json-0.6.4-py3-none-any.whl (28 kB)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Collecting langchain-community<0.1,>=0.0.21 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langchain_community-0.0.24-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m73.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langchain-core<0.2,>=0.1.26 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langchain_core-0.1.26-py3-none-any.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m246.4/246.4 kB\u001b[0m \u001b[31m31.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting langsmith<0.2.0,>=0.1.0 (from langchain->-r requirements.txt (line 5))\n",
            "  Downloading langsmith-0.1.8-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.2/62.2 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pydantic<3,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (2.6.1)\n",
            "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from langchain->-r requirements.txt (line 5)) (8.2.3)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 6)) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface_hub->-r requirements.txt (line 6)) (4.9.0)\n",
            "Collecting deprecation (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting pylance==0.9.18 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading pylance-0.9.18-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.6/21.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ratelimiter~=1.0 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading ratelimiter-1.2.0.post0-py3-none-any.whl (6.6 kB)\n",
            "Collecting retry>=0.9.2 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading retry-0.9.2-py2.py3-none-any.whl (8.0 kB)\n",
            "Requirement already satisfied: attrs>=21.3.0 in /usr/local/lib/python3.10/dist-packages (from lancedb->-r requirements.txt (line 7)) (23.2.0)\n",
            "Collecting semver>=3.0 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading semver-3.0.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cachetools in /usr/local/lib/python3.10/dist-packages (from lancedb->-r requirements.txt (line 7)) (5.3.2)\n",
            "Collecting overrides>=0.7 (from lancedb->-r requirements.txt (line 7))\n",
            "  Downloading overrides-7.7.0-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: torch>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (2.1.0+cu121)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.2.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (1.11.4)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from sentence-transformers->-r requirements.txt (line 9)) (9.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate->-r requirements.txt (line 11)) (5.9.5)\n",
            "Collecting types-requests<3.0.0.0,>=2.31.0.2 (from langchainhub->-r requirements.txt (line 12))\n",
            "  Downloading types_requests-2.31.0.20240218-py3-none-any.whl (14 kB)\n",
            "Collecting diskcache>=5.6.1 (from llama_cpp_python->-r requirements.txt (line 13))\n",
            "  Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.3.1)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain->-r requirements.txt (line 5)) (1.9.4)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading marshmallow-3.20.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.4/49.4 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-inspect<1,>=0.4.0 (from dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->-r requirements.txt (line 1)) (2.1.5)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain->-r requirements.txt (line 5))\n",
            "  Downloading jsonpointer-2.4-py2.py3-none-any.whl (7.8 kB)\n",
            "Requirement already satisfied: anyio<5,>=3 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.2,>=0.1.26->langchain->-r requirements.txt (line 5)) (3.7.1)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.0->langchain->-r requirements.txt (line 5))\n",
            "  Downloading orjson-3.9.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (138 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m138.5/138.5 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 5)) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.16.2 in /usr/local/lib/python3.10/dist-packages (from pydantic<3,>=1->langchain->-r requirements.txt (line 5)) (2.16.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->-r requirements.txt (line 3)) (1.16.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers->-r requirements.txt (line 4)) (2024.2.2)\n",
            "Requirement already satisfied: decorator>=3.4.2 in /usr/local/lib/python3.10/dist-packages (from retry>=0.9.2->lancedb->-r requirements.txt (line 7)) (4.4.2)\n",
            "Collecting py<2.0.0,>=1.4.26 (from retry>=0.9.2->lancedb->-r requirements.txt (line 7))\n",
            "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain->-r requirements.txt (line 5)) (3.0.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (3.2.1)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (2.1.0)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->sentence-transformers->-r requirements.txt (line 9)) (3.3.0)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain->-r requirements.txt (line 5)) (1.3.0)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3->langchain-core<0.2,>=0.1.26->langchain->-r requirements.txt (line 5)) (1.2.0)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json<0.7,>=0.5.7->langchain->-r requirements.txt (line 5))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.11.0->sentence-transformers->-r requirements.txt (line 9)) (1.3.0)\n",
            "Building wheels for collected packages: llama_cpp_python\n",
            "  Building wheel for llama_cpp_python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for llama_cpp_python: filename=llama_cpp_python-0.2.52-cp310-cp310-manylinux_2_35_x86_64.whl size=25193288 sha256=24b176592c849ea4656340c52a8ca2173fcb22b3afb8929d759a89aaf1652d23\n",
            "  Stored in directory: /root/.cache/pip/wheels/b7/a7/b5/61408aace30f6fae3999405ae973a8280a8ca1f20cb0ccdcc3\n",
            "Successfully built llama_cpp_python\n",
            "Installing collected packages: ratelimiter, types-requests, semver, rank_bm25, py, overrides, orjson, mypy-extensions, marshmallow, jsonpointer, diskcache, deprecation, typing-inspect, retry, pylance, llama_cpp_python, langchainhub, jsonpatch, bitsandbytes, langsmith, lancedb, dataclasses-json, accelerate, langchain-core, sentence-transformers, langchain-community, langchain\n",
            "Successfully installed accelerate-0.27.2 bitsandbytes-0.42.0 dataclasses-json-0.6.4 deprecation-2.1.0 diskcache-5.6.3 jsonpatch-1.33 jsonpointer-2.4 lancedb-0.5.7 langchain-0.1.9 langchain-community-0.0.24 langchain-core-0.1.26 langchainhub-0.1.14 langsmith-0.1.8 llama_cpp_python-0.2.52 marshmallow-3.20.2 mypy-extensions-1.0.0 orjson-3.9.15 overrides-7.7.0 py-1.11.0 pylance-0.9.18 rank_bm25-0.2.2 ratelimiter-1.2.0.post0 retry-0.9.2 semver-3.0.2 sentence-transformers-2.4.0 types-requests-2.31.0.20240218 typing-inspect-0.9.0\n"
          ]
        }
      ],
      "source": [
        "! pip install -r requirements.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Cibu1RWm3FdA"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\akaga\\Python_for_classes\\env_for_ml_st\\lib\\site-packages\\deeplake\\util\\check_latest_version.py:32: UserWarning: A newer version of deeplake (3.8.22) is available. It's recommended that you update to the latest version using `pip install -U deeplake`.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'pwd'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_16760/2618484233.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectorstores\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlancedb\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLanceDB\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrievers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBM25Retriever\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrievers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mEnsembleRetriever\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mschema\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDocument\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\akaga\\Python_for_classes\\env_for_ml_st\\lib\\site-packages\\langchain\\retrievers\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     35\u001b[0m     \u001b[0mTimeWeightedVectorStoreRetriever\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m )\n\u001b[1;32m---> 37\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrievers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweb_research\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWebResearchRetriever\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     38\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minteractive_env\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mis_interactive_env\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\akaga\\Python_for_classes\\env_for_ml_st\\lib\\site-packages\\langchain\\retrievers\\web_research.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtyping\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mList\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mOptional\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mAsyncHtmlLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_transformers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHtml2TextTransformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mllms\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mLlamaCpp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\akaga\\Python_for_classes\\env_for_ml_st\\lib\\site-packages\\langchain_community\\document_loaders\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    161\u001b[0m     \u001b[0mUnstructuredPDFLoader\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m )\n\u001b[1;32m--> 163\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpebblo\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPebbloSafeLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpolars_dataframe\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mPolarsDataFrameLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mlangchain_community\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdocument_loaders\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpowerpoint\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mUnstructuredPowerPointLoader\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32mc:\\Users\\akaga\\Python_for_classes\\env_for_ml_st\\lib\\site-packages\\langchain_community\\document_loaders\\pebblo.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mlogging\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mpwd\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0muuid\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mhttp\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mHTTPStatus\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pwd'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import lancedb\n",
        "from torch import cuda\n",
        "import urllib.request\n",
        "\n",
        "from langchain.retrievers import EnsembleRetriever\n",
        "from langchain.schema import Document\n",
        "from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
        "from langchain.callbacks.manager import CallbackManager\n",
        "from langchain.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
        "\n",
        "from langchain_community.vectorstores.lancedb import LanceDB\n",
        "from langchain_community.retrievers import BM25Retriever\n",
        "from langchain_community.llms import LlamaCpp\n",
        "\n",
        "from langchain_core.documents.base import Document\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_core.runnables import RunnablePassthrough, RunnablePick\n",
        "from langchain_core.prompts import ChatPromptTemplate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "vHJj7lZ33yaJ"
      },
      "outputs": [],
      "source": [
        "# Remove db if something changed in the structure\n",
        "# !rm -rf /app/db"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Settings to run the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "mW5We1j23tr7"
      },
      "outputs": [],
      "source": [
        "path_to_data_csv = 'master_without_embeddings_first_100.csv'\n",
        "\n",
        "path_to_database = '/app/db'\n",
        "\n",
        "embedding_model = 'sentence-transformers/all-MiniLM-L6-v2'\n",
        "\n",
        "HF_AUTH = os.getenv('HF_AUTH', None)\n",
        "os.environ['HF_HOME'] = os.getenv('HF_HOME', 'models')\n",
        "model_id='llama-2-7b-chat.Q2_K.gguf'\n",
        "\n",
        "chunk_size = 400\n",
        "chunk_overlap = 50\n",
        "\n",
        "retrieve_top_k_docs_bm25 = 1\n",
        "retrieve_top_k_docs_vector = 1\n",
        "context_length_for_llm = chunk_size*(retrieve_top_k_docs_bm25 + retrieve_top_k_docs_vector)+200 #not larger than 2048\n",
        "retrievers_weights_bm25 = 0.4 #probability\n",
        "llama_temperature = 0.75 #randomness parameter"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Load the data into type Document"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KKdqjVrr3yqp",
        "outputId": "edff801b-e021-4c55-d917-9e59df2f961f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "--- Read 412 documents from master_without_embeddings_first_100.csv\n"
          ]
        }
      ],
      "source": [
        "df = pd.read_csv(path_to_data_csv)\n",
        "\n",
        "documents=[]\n",
        "for index, row in df.iterrows():\n",
        "    doc = Document(page_content = row['chunk'],\n",
        "                   metadata={'id': row['id'], 'title': row['title'], 'authors': row['authors'], 'sources': row['sources']})\n",
        "    documents.append(doc)\n",
        "\n",
        "print(f'---\\n--- Read {len(documents)} documents from {path_to_data_csv}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create BM25- and LanceDB retrievers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "77vI84Cn32Rq",
        "outputId": "2efe824d-ad56-483a-9189-5e57af2afb0b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---\n",
            "--- Creating retrievers...\n",
            "--- Trying to connect to LanceDB\n",
            "--- Error connecting to LanceDB, creating new one\n",
            "--- LanceDB created and connected successfully\n",
            "--- Finished loading documents to LanceDB\n",
            "---\n",
            "--- Created BM25 and vector search retrievers\n"
          ]
        }
      ],
      "source": [
        "print(f'---\\n--- Creating retrievers...')\n",
        "\n",
        "bm25_retriever = BM25Retriever.from_documents(documents)\n",
        "bm25_retriever.k =  retrieve_top_k_docs_bm25\n",
        "\n",
        "device = 'cuda' if cuda.is_available() else 'cpu'\n",
        "\n",
        "# Create embedding\n",
        "embed_model = HuggingFaceEmbeddings(\n",
        "    model_name=embedding_model,\n",
        "    model_kwargs={'device': device},\n",
        "    encode_kwargs={'device': device, 'batch_size': 32}\n",
        ")\n",
        "\n",
        "# Try if the LanceDB exists, if yes, use if, if no, create new one\n",
        "try:\n",
        "    print(\"--- Trying to connect to LanceDB\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.open_table(\"chatmaja_test\")\n",
        "    docsearch = LanceDB(connection=table, embedding=embed_model)\n",
        "    print(\"--- LanceDB found, connected successfully\")\n",
        "except:\n",
        "    print(\"--- Error connecting to LanceDB, creating new one\")\n",
        "    db = lancedb.connect(path_to_database)\n",
        "    table = db.create_table(\"chatmaja_test\", data=[\n",
        "            {\"vector\": embed_model.embed_query(\"Hello World\"), \"text\": \"Hello World\", \"id\": \"1\", \"authors\": \"authoors\", \"sources\": \"sourcees\", \"title\": \"tiitle\"}\n",
        "        ], mode=\"overwrite\")\n",
        "    print(\"--- LanceDB created and connected successfully\")\n",
        "    docsearch = LanceDB.from_documents(documents, embed_model, connection=table)\n",
        "    print(\"--- Finished loading documents to LanceDB\")\n",
        "\n",
        "retriever_lancedb = docsearch.as_retriever(search_kwargs={\"k\": retrieve_top_k_docs_vector})\n",
        "\n",
        "# Create ensemble retriver\n",
        "ensemble_retriever = EnsembleRetriever(retrievers=[bm25_retriever, retriever_lancedb],\n",
        "                                       weights=[retrievers_weights_bm25, 1-retrievers_weights_bm25])\n",
        "\n",
        "print(\"---\\n--- Created BM25 and vector search retrievers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Get model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create directory if it does not exist\n",
        "os.makedirs(os.getenv('HF_HOME'), exist_ok=True)\n",
        "\n",
        "# Download model if not exists\n",
        "path_to_model = os.path.join(os.getenv('HF_HOME'), model_id)\n",
        "link_to_model = f\"https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF/resolve/main/{model_id}\"\n",
        "\n",
        "if not os.path.isfile(path_to_model):\n",
        "    print(f\"--- Downloading {model_id}...\")\n",
        "    urllib.request.urlretrieve(link_to_model, path_to_model)\n",
        "    print(f\"--- Downloaded {model_id} successfully.\")\n",
        "else:\n",
        "    print(f\"--- Model {model_id} already downloaded.\")\n",
        "\n",
        "\n",
        "# Callbacks support token-wise streaming\n",
        "callback_manager = CallbackManager([StreamingStdOutCallbackHandler()])\n",
        "\n",
        "# Make sure the model path is correct for your system!\n",
        "n_gpu_layers = -1 if device == 'cuda' else 0\n",
        "llm = LlamaCpp(\n",
        "    model_path=path_to_model,\n",
        "    temperature=llama_temperature,\n",
        "    max_tokens=min(context_length_for_llm*2, 4096),\n",
        "    n_gpu_layers=n_gpu_layers,\n",
        "    n_ctx=min(context_length_for_llm, 2048),\n",
        "    top_p=1,\n",
        "    callback_manager=callback_manager,\n",
        "    verbose=True,  # Verbose is required to pass to the callback manager\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Create pipeline of the solution"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rO-k6km5KVV",
        "outputId": "58d41c91-1f7a-4f4e-c013-dc56a3a038ee"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llama_model_loader: loaded meta data with 19 key-value pairs and 291 tensors from models/llama-2-7b-chat.Q2_K.gguf (version GGUF V2)\n",
            "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
            "llama_model_loader: - kv   0:                       general.architecture str              = llama\n",
            "llama_model_loader: - kv   1:                               general.name str              = LLaMA v2\n",
            "llama_model_loader: - kv   2:                       llama.context_length u32              = 4096\n",
            "llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n",
            "llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n",
            "llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n",
            "llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n",
            "llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n",
            "llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n",
            "llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000001\n",
            "llama_model_loader: - kv  10:                          general.file_type u32              = 10\n",
            "llama_model_loader: - kv  11:                       tokenizer.ggml.model str              = llama\n",
            "llama_model_loader: - kv  12:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
            "llama_model_loader: - kv  13:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n",
            "llama_model_loader: - kv  14:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
            "llama_model_loader: - kv  15:                tokenizer.ggml.bos_token_id u32              = 1\n",
            "llama_model_loader: - kv  16:                tokenizer.ggml.eos_token_id u32              = 2\n",
            "llama_model_loader: - kv  17:            tokenizer.ggml.unknown_token_id u32              = 0\n",
            "llama_model_loader: - kv  18:               general.quantization_version u32              = 2\n",
            "llama_model_loader: - type  f32:   65 tensors\n",
            "llama_model_loader: - type q2_K:   65 tensors\n",
            "llama_model_loader: - type q3_K:  160 tensors\n",
            "llama_model_loader: - type q6_K:    1 tensors\n",
            "llm_load_vocab: special tokens definition check successful ( 259/32000 ).\n",
            "llm_load_print_meta: format           = GGUF V2\n",
            "llm_load_print_meta: arch             = llama\n",
            "llm_load_print_meta: vocab type       = SPM\n",
            "llm_load_print_meta: n_vocab          = 32000\n",
            "llm_load_print_meta: n_merges         = 0\n",
            "llm_load_print_meta: n_ctx_train      = 4096\n",
            "llm_load_print_meta: n_embd           = 4096\n",
            "llm_load_print_meta: n_head           = 32\n",
            "llm_load_print_meta: n_head_kv        = 32\n",
            "llm_load_print_meta: n_layer          = 32\n",
            "llm_load_print_meta: n_rot            = 128\n",
            "llm_load_print_meta: n_embd_head_k    = 128\n",
            "llm_load_print_meta: n_embd_head_v    = 128\n",
            "llm_load_print_meta: n_gqa            = 1\n",
            "llm_load_print_meta: n_embd_k_gqa     = 4096\n",
            "llm_load_print_meta: n_embd_v_gqa     = 4096\n",
            "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
            "llm_load_print_meta: f_norm_rms_eps   = 1.0e-06\n",
            "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
            "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
            "llm_load_print_meta: n_ff             = 11008\n",
            "llm_load_print_meta: n_expert         = 0\n",
            "llm_load_print_meta: n_expert_used    = 0\n",
            "llm_load_print_meta: pooling type     = 0\n",
            "llm_load_print_meta: rope type        = 0\n",
            "llm_load_print_meta: rope scaling     = linear\n",
            "llm_load_print_meta: freq_base_train  = 10000.0\n",
            "llm_load_print_meta: freq_scale_train = 1\n",
            "llm_load_print_meta: n_yarn_orig_ctx  = 4096\n",
            "llm_load_print_meta: rope_finetuned   = unknown\n",
            "llm_load_print_meta: model type       = 7B\n",
            "llm_load_print_meta: model ftype      = Q2_K - Medium\n",
            "llm_load_print_meta: model params     = 6.74 B\n",
            "llm_load_print_meta: model size       = 2.63 GiB (3.35 BPW) \n",
            "llm_load_print_meta: general.name     = LLaMA v2\n",
            "llm_load_print_meta: BOS token        = 1 '<s>'\n",
            "llm_load_print_meta: EOS token        = 2 '</s>'\n",
            "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
            "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
            "llm_load_tensors: ggml ctx size =    0.22 MiB\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--- Model llama-2-7b-chat.Q2_K.gguf already downloaded.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "llm_load_tensors: offloading 32 repeating layers to GPU\n",
            "llm_load_tensors: offloading non-repeating layers to GPU\n",
            "llm_load_tensors: offloaded 33/33 layers to GPU\n",
            "llm_load_tensors:        CPU buffer size =    41.02 MiB\n",
            "llm_load_tensors:      CUDA0 buffer size =  2653.31 MiB\n",
            ".................................................................................................\n",
            "llama_new_context_with_model: n_ctx      = 1000\n",
            "llama_new_context_with_model: freq_base  = 10000.0\n",
            "llama_new_context_with_model: freq_scale = 1\n",
            "llama_kv_cache_init:      CUDA0 KV buffer size =   500.00 MiB\n",
            "llama_new_context_with_model: KV self size  =  500.00 MiB, K (f16):  250.00 MiB, V (f16):  250.00 MiB\n",
            "llama_new_context_with_model:  CUDA_Host input buffer size   =     0.16 MiB\n",
            "llama_new_context_with_model:      CUDA0 compute buffer size =     1.51 MiB\n",
            "llama_new_context_with_model:  CUDA_Host compute buffer size =     0.12 MiB\n",
            "llama_new_context_with_model: graph splits (measure): 2\n",
            "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | \n",
            "Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.context_length': '4096', 'general.name': 'LLaMA v2', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000001', 'llama.rope.dimension_count': '128', 'llama.attention.head_count': '32', 'tokenizer.ggml.bos_token_id': '1', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n"
          ]
        }
      ],
      "source": [
        "def format_docs(docs):\n",
        "  return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# Prompt\n",
        "rag_prompt_llama = ChatPromptTemplate.from_messages([\n",
        "    (\"human\", \"\"\"[INST]<<SYS>> You are an assistant for ques\n",
        "     tion-answering tasks.\n",
        "    Use the following pieces of retrieved context to answer the question.\n",
        "    If you don't know the answer, just say that you don't know.\n",
        "    Use three sentences maximum and keep the answer concise.<</SYS>> \\nQuestion: {question} \\nContext: {context} \\nAnswer: [/INST]\"\"\"),\n",
        "])\n",
        "\n",
        "# Chain\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(context=RunnablePick(\"context\") | format_docs)\n",
        "    | rag_prompt_llama\n",
        "    | llm\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "def answer_query(question):\n",
        "    \"\"\"\n",
        "    Get answer for provided question.\n",
        "\n",
        "    Args:\n",
        "        question (str): question from the user.\n",
        "    \"\"\"\n",
        "    print(f'- - - Question: {question}')\n",
        "    docs = ensemble_retriever.get_relevant_documents(question)\n",
        "    print(f'- - - Relevant documents: {[d.page_content for d in docs]}')\n",
        "    result = chain.invoke({\"context\": docs, \"question\": question})\n",
        "    print(f'- - - Results: {result}')\n",
        "    answer = f\"Query: {question}\\n\\nAnswer: {result}\"\n",
        "    return answer, docs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Sample usage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UcT2TVk7gLw",
        "outputId": "2cda68d6-4f87-4d4c-c7d5-1a1b5dfb59a8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- - - Question: What is used in brain cancer imaging?\n",
            "- - - Relevant documents: [\"Two independent reviewers screened abstracts, titles and full text, resolving differences through discussion. RESULTS: 228 studies met the criteria. XAI publications are increasing, targeting MRI (n = 73), radiography (n = 47), CT (n = 46). Lung (n = 82) and brain (n = 74) pathologies, Covid-19 (n = 48), Alzheimer's disease (n = 25), brain tumors (n = 15) are the main pathologies explained. Explanations are presented visually (n = 186),\", 'BACKGROUND: Transformer-based models are gaining popularity in medical imaging and cancer imaging applications. Many recent studies have demonstrated the use of transformer-based models for brain cancer imaging applications such as diagnosis and tumor segmentation. OBJECTIVE: This study aims to review how different vision transformers (ViTs) contributed to advancing brain cancer diagnosis and tumor segmentation using brain image data. This study examines the different architectures developed for enhancing the task of brain tumor segmentation. Furthermore, it explores']\n",
            "  Based on the provided context, it appears that transformer-based models have been increasingly used in brain cancer imaging for various tasks such as diagnosis and tumor segmentation. Specifically, the study found that:\n",
            "* 74% of the studies used transformer-based models for brain cancer diagnosis, while 55% used them for tumor segmentation.\n",
            "* The most common type of transformer used was the ViT, which was employed by 82% of the studies.\n",
            "* The study found that transformer-based models were particularly useful for analyzing brain tumors, with 70% of the studies using them for this purpose.\n",
            "* In terms of visual explanations, 186 were provided in the studies reviewed, with the majority (72%) being images.\n",
            "Overall, the study suggests that transformer-based models have shown promise in advancing brain cancer diagnosis and tumor segmentation using brain image data, and may be a valuable tool in the field of medical imaging. However, it is important to note that the study was based on a limited sample of 228 studies, and further research is needed to fully understand the potential of transformer-based models in this area."
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "llama_print_timings:        load time =      84.83 ms\n",
            "llama_print_timings:      sample time =     144.12 ms /   263 runs   (    0.55 ms per token,  1824.87 tokens per second)\n",
            "llama_print_timings: prompt eval time =    4236.79 ms /   370 tokens (   11.45 ms per token,    87.33 tokens per second)\n",
            "llama_print_timings:        eval time =   10972.62 ms /   262 runs   (   41.88 ms per token,    23.88 tokens per second)\n",
            "llama_print_timings:       total time =   16814.59 ms /   632 tokens\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "- - - Results:   Based on the provided context, it appears that transformer-based models have been increasingly used in brain cancer imaging for various tasks such as diagnosis and tumor segmentation. Specifically, the study found that:\n",
            "* 74% of the studies used transformer-based models for brain cancer diagnosis, while 55% used them for tumor segmentation.\n",
            "* The most common type of transformer used was the ViT, which was employed by 82% of the studies.\n",
            "* The study found that transformer-based models were particularly useful for analyzing brain tumors, with 70% of the studies using them for this purpose.\n",
            "* In terms of visual explanations, 186 were provided in the studies reviewed, with the majority (72%) being images.\n",
            "Overall, the study suggests that transformer-based models have shown promise in advancing brain cancer diagnosis and tumor segmentation using brain image data, and may be a valuable tool in the field of medical imaging. However, it is important to note that the study was based on a limited sample of 228 studies, and further research is needed to fully understand the potential of transformer-based models in this area.\n"
          ]
        }
      ],
      "source": [
        "query = \"What is used in brain cancer imaging?\"\n",
        "answer, docs = answer_query(query)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "Sga6L5bIzaay",
        "outputId": "9502ed73-6544-4d09-b4c6-5dbf26d78e7d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'https://pubmed.ncbi.nlm.nih.gov/37976760/'"
            ]
          },
          "execution_count": 63,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get link to PubMed of first document.\n",
        "docs[0].metadata['sources']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "40eGFKSt_IhN",
        "outputId": "d815371f-8b09-41f8-bb7e-0027b0866526"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'A scoping review of interpretability and explainability concerning artificial intelligence methods in medical imaging.'"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get title of first document.\n",
        "docs[0].metadata['title']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "MhMIiZlN_5nx",
        "outputId": "e1f83f83-c87d-42d9-eb9d-6b7d6a95031b"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Champendal M||Muller H||Prior JO||Dos Reis CS'"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Get authors of first document.\n",
        "docs[0].metadata['authors']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "5jQXxGWn1i12",
        "outputId": "59adcb65-9d2a-4929-9c5f-e24c21550089"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Query: What is used in brain cancer imaging?\\n\\nAnswer:   Based on the provided context, here is the answer to the question \"What is used in brain cancer imaging?\"\\nVision Transformers (ViTs) have gained popularity in medical imaging and cancer imaging applications, including brain cancer diagnosis and tumor segmentation. Many recent studies have demonstrated the use of transformer-based models for brain cancer imaging applications such as diagnosis and tumor segmentation. These models have contributed significantly to advancing brain cancer diagnosis and tumor segmentation using brain image data. The study examines different architectures developed for enhancing the task of brain tumor segmentation and explores their effectiveness in brain cancer imaging.\\nIn summary, ViTs are increasingly being used in brain cancer imaging for tasks such as diagnosis and tumor segmentation, and have shown promising results in improving these tasks.\\nSources:\\n1. \"A Survey on Vision Transformers in Medical Imaging\" by M. A. H. Nour, et al. (2022). DOI: 10.1007/s12302-022-00027-x\\n2. \"Vision Transformers for Brain Tumor Segmentation: A Survey\" by A. S. R. K. Rao, et al. (2022). DOI: 10.1007/s12302-022-00026-z\\n3. \"Brain Tumor Segmentation using Vision Transformers: A Comprehensive Review\" by S. B. Dandin and S. R. K. Rao (2022). DOI: 10.1007/s12302-022-00025-w'"
            ]
          },
          "execution_count": 50,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Print answer generated by llama.\n",
        "answer"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
